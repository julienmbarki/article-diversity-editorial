{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('df_etudiants.xlsx')\n",
    "\n",
    "# Group by playlist name\n",
    "groups = df.groupby('playlist_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gap(data, k):\n",
    "  \"\"\"\n",
    "  Compute the gap statistic for a given value of k.\n",
    "\n",
    "  Parameters:\n",
    "  - data: the data to cluster, with shape (n_samples, n_features)\n",
    "  - k: the number of clusters\n",
    "\n",
    "  Returns:\n",
    "  - gap: the gap statistic for the given value of k\n",
    "  \"\"\"\n",
    "  # Compute the WCSS for the real data\n",
    "  kmeans = KMeans(n_clusters=k, random_state=42).fit(data)\n",
    "  wcss = kmeans.inertia_\n",
    "\n",
    "  # Compute the null reference distribution by shuffling the data and\n",
    "  # re-assigning it to clusters\n",
    "  n_samples, n_features = data.shape\n",
    "  wcss_null = []\n",
    "\n",
    "  for _ in range(20):\n",
    "    data_shuffled = np.random.permutation(data)\n",
    "    wcss_null.append(KMeans(n_clusters=k).fit(data_shuffled).inertia_)\n",
    "  \n",
    "  wcss_null = np.array(wcss_null)\n",
    "  \n",
    "  # Compute the gap statistic and gap*\n",
    "  gap = np.log(np.mean(wcss_null)) - np.log(wcss)\n",
    "\n",
    "  # Compute the standard deviation of the null reference distribution\n",
    "  gap_std = np.std(np.log(wcss_null))\n",
    "\n",
    "  return gap, gap_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Initialize dictionaries to store clustering results and optimal ks\n",
    "reduced_data_dict_6 = {}\n",
    "reduced_data_dict_5 = {}\n",
    "reduced_data_dict_4 = {}\n",
    "\n",
    "optimal_num_k_6 = {}\n",
    "optimal_num_k_5 = {}\n",
    "optimal_num_k_4 = {}\n",
    "\n",
    "optimal_num_k_2_6 = {}\n",
    "optimal_num_k_2_5 = {}\n",
    "optimal_num_k_2_4 = {}\n",
    "\n",
    "# Initialize PCA objects\n",
    "pca_6 = PCA(n_components=6)\n",
    "pca_5 = PCA(n_components=5)\n",
    "pca_4 = PCA(n_components=4)\n",
    "\n",
    "# Loop over each group and apply PCA with the specified number of components\n",
    "for group_name, group in groups:\n",
    "    # Subset data\n",
    "    subset = group.loc[:, \"danceability\":\"duration_ms\"]\n",
    "\n",
    "    scaled_columns = scaler.fit_transform(subset)\n",
    "\n",
    "    # Apply PCA \n",
    "    reduced_data_6 = pca_6.fit_transform(scaled_columns)\n",
    "    reduced_data_5 = pca_5.fit_transform(scaled_columns)\n",
    "    reduced_data_4 = pca_4.fit_transform(scaled_columns)\n",
    "\n",
    "    reduced_data_dict_6[group_name] = reduced_data_6\n",
    "    reduced_data_dict_5[group_name] = reduced_data_5\n",
    "    reduced_data_dict_4[group_name] = reduced_data_4\n",
    "\n",
    "    # Initialize lists to store the gap statistics and error bars for different values of k\n",
    "    group_gaps_6 = []\n",
    "    group_errors_6 = []\n",
    "\n",
    "    group_gaps_5 = []\n",
    "    group_errors_5 = []\n",
    "\n",
    "    group_gaps_4 = []\n",
    "    group_errors_4 = []\n",
    "\n",
    "    # Loop over different values of k\n",
    "    for k in range(1, 11):\n",
    "        # Compute the gap statistic and standard deviation for the current value of k\n",
    "        gap_6, gap_std_6 = compute_gap(reduced_data_6, k)\n",
    "        group_gaps_6.append(gap_6)\n",
    "        group_errors_6.append(gap_std_6)\n",
    "\n",
    "        gap_5, gap_std_5 = compute_gap(reduced_data_5, k)\n",
    "        group_gaps_5.append(gap_5)\n",
    "        group_errors_5.append(gap_std_5)\n",
    "\n",
    "        gap_4, gap_std_4 = compute_gap(reduced_data_4, k)\n",
    "        group_gaps_4.append(gap_4)\n",
    "        group_errors_4.append(gap_std_4)\n",
    "\n",
    "    # Find the optimal number of components based on the gap statistic criterion\n",
    "    optimal_k_6 = None\n",
    "    for i in range(1, len(group_gaps_6) - 1):\n",
    "        s_k = group_errors_6[i]\n",
    "        threshold = s_k * np.sqrt(1 + 1 / 20)\n",
    "        if group_gaps_6[i] >= group_gaps_6[i + 1] - threshold:\n",
    "            optimal_k_6 = i + 1\n",
    "            break\n",
    "\n",
    "    if optimal_k_6 is None:\n",
    "        optimal_k_6 = np.argmax(group_gaps_6) + 1\n",
    "\n",
    "    optimal_k_5 = None\n",
    "    for i in range(1, len(group_gaps_5) - 1):\n",
    "        s_k = group_errors_5[i]\n",
    "        threshold = s_k * np.sqrt(1 + 1 / 20)\n",
    "        if group_gaps_5[i] >= group_gaps_5[i + 1] - threshold:\n",
    "            optimal_k_5 = i + 1\n",
    "            break\n",
    "\n",
    "    if optimal_k_5 is None:\n",
    "        optimal_k_5 = np.argmax(group_gaps_5) + 1\n",
    "\n",
    "    optimal_k_4 = None\n",
    "    for i in range(1, len(group_gaps_4) - 1):\n",
    "        s_k = group_errors_4[i]\n",
    "        threshold = s_k * np.sqrt(1 + 1 / 20)\n",
    "        if group_gaps_4[i] >= group_gaps_4[i + 1] - threshold:\n",
    "            optimal_k_4 = i + 1\n",
    "            break\n",
    "\n",
    "    if optimal_k_4 is None:\n",
    "        optimal_k_4 = np.argmax(group_gaps_4) + 1\n",
    "\n",
    "    optimal_num_k_6[group_name] = optimal_k_6\n",
    "    optimal_num_k_5[group_name] = optimal_k_5\n",
    "    optimal_num_k_4[group_name] = optimal_k_4\n",
    "\n",
    "    # Find the optimal number of components based on the gap* statistic criterion\n",
    "    optimal_k_2_6 = np.argmax(group_gaps_6) + 1\n",
    "    optimal_k_2_5 = np.argmax(group_gaps_5) + 1\n",
    "    optimal_k_2_4 = np.argmax(group_gaps_4) + 1\n",
    "\n",
    "    optimal_num_k_2_6[group_name] = optimal_k_2_6\n",
    "    optimal_num_k_2_5[group_name] = optimal_k_2_5\n",
    "    optimal_num_k_2_4[group_name] = optimal_k_2_4\n",
    "\n",
    "    print(f\"Group: {group_name}, Optimal k 6 (gap): {optimal_num_k_6[group_name]}, Optimal k 6 (gap*): {optimal_num_k_2_6[group_name]} and Optimal k 5 (gap): {optimal_num_k_5[group_name]}, Optimal k 5 (gap*): {optimal_num_k_2_5[group_name]}, Optimal k 4 (gap): {optimal_num_k_4[group_name]},Optimal k 4 (gap*): {optimal_num_k_2_4[group_name]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Loop over each group and apply PCA with the specified number of components\n",
    "for group_name, group in groups:\n",
    "    try: \n",
    "        # Get reduced data\n",
    "        reduced_data_6 = reduced_data_dict_6[group_name]\n",
    "        reduced_data_5 = reduced_data_dict_5[group_name]\n",
    "        reduced_data_4 = reduced_data_dict_4[group_name]\n",
    "\n",
    "        # Perform k-means clustering for both configurations\n",
    "        kmeans_6 = KMeans(n_clusters=optimal_num_k_6[group_name], random_state=42).fit(reduced_data_6)\n",
    "        kmeans_2_6 = KMeans(n_clusters=optimal_num_k_2_6[group_name], random_state=42).fit(reduced_data_6)\n",
    "\n",
    "        kmeans_5 = KMeans(n_clusters=optimal_num_k_5[group_name], random_state=42).fit(reduced_data_5)\n",
    "        kmeans_2_5 = KMeans(n_clusters=optimal_num_k_2_5[group_name], random_state=42).fit(reduced_data_5)\n",
    "\n",
    "        kmeans_4 = KMeans(n_clusters=optimal_num_k_4[group_name], random_state=42).fit(reduced_data_4)\n",
    "        kmeans_2_4 = KMeans(n_clusters=optimal_num_k_2_4[group_name], random_state=42).fit(reduced_data_4)\n",
    "\n",
    "        # Get cluster assignments for both configurations\n",
    "        cluster_assignments_6 = kmeans_6.labels_\n",
    "        cluster_assignments_2_6 = kmeans_2_6.labels_\n",
    "\n",
    "        cluster_assignments_5 = kmeans_5.labels_\n",
    "        cluster_assignments_2_5 = kmeans_2_5.labels_\n",
    "\n",
    "        cluster_assignments_4 = kmeans_4.labels_\n",
    "        cluster_assignments_2_4 = kmeans_2_4.labels_\n",
    "\n",
    "        # Compute silhouette scores for both configurations\n",
    "        silhouette_score_6 = silhouette_score(reduced_data_6, cluster_assignments_6)\n",
    "        silhouette_score_2_6 = silhouette_score(reduced_data_6, cluster_assignments_2_6)\n",
    "\n",
    "        silhouette_score_5 = silhouette_score(reduced_data_5, cluster_assignments_5)\n",
    "        silhouette_score_2_5 = silhouette_score(reduced_data_5, cluster_assignments_2_5)\n",
    "\n",
    "        silhouette_score_4 = silhouette_score(reduced_data_4, cluster_assignments_4)\n",
    "        silhouette_score_2_4 = silhouette_score(reduced_data_4, cluster_assignments_2_4)\n",
    "\n",
    "        results_df = results_df.append({\n",
    "            'playlist_name': group_name,\n",
    "            'nb_cluster_6': optimal_num_k_6[group_name],\n",
    "            'nb_cluster_6_2': optimal_num_k_2_6[group_name],\n",
    "            'silhouette_score_6': silhouette_score_6,\n",
    "            'silhouette_score_6_2': silhouette_score_2_6,\n",
    "            'nb_cluster_5': optimal_num_k_5[group_name],\n",
    "            'nb_cluster_5_2': optimal_num_k_2_5[group_name],\n",
    "            'silhouette_score_5': silhouette_score_5,\n",
    "            'silhouette_score_5_2': silhouette_score_2_5,\n",
    "            'nb_cluster_4': optimal_num_k_4[group_name],\n",
    "            'nb_cluster_4_2': optimal_num_k_2_4[group_name],\n",
    "            'silhouette_score_4': silhouette_score_4,\n",
    "            'silhouette_score_4_2': silhouette_score_2_4\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing group {group_name}: {e}\")\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel('robust_etudiants.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
