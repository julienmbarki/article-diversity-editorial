{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and data management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('df_tops.xlsx')\n",
    "\n",
    "# Group by playlist name\n",
    "groups = df.groupby('playlist_name')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the number of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Store optimal number of components for each group\n",
    "optimal_num_components = []\n",
    "\n",
    "for group_name, group in groups:\n",
    "    # Subset data\n",
    "    subset = group.loc[:, \"danceability\":\"duration_ms\"]\n",
    "    \n",
    "    # Scale the specified columns\n",
    "    scaled_columns = scaler.fit_transform(subset)\n",
    "\n",
    "    # Perform PCA to reduce dimensionality of data\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_columns)\n",
    "    \n",
    "    # Determine the optimal number of components\n",
    "    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "    optimal_components = next(i for i, var in enumerate(cumulative_variance) if var >= 0.8) + 1\n",
    "    optimal_num_components.append(optimal_components)\n",
    "\n",
    "    # Plot scree plot\n",
    "    plt.plot(range(1, pca.n_components_ + 1), cumulative_variance, 'bo-', linewidth=2)\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title(group_name)\n",
    "    plt.show()\n",
    "\n",
    "# Calculate the most modal value\n",
    "most_common_optimal = Counter(optimal_num_components).most_common(1)\n",
    "most_modal_value = most_common_optimal[0][0]\n",
    "\n",
    "print(\"Most modal value of optimal components:\", most_modal_value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each group and apply PCA with optimal number of components\n",
    "reduced_data_dict = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "    # Subset data\n",
    "    subset = group.loc[:, \"danceability\":\"duration_ms\"]\n",
    "\n",
    "    # Scale the specified columns\n",
    "    scaled_columns = scaler.fit_transform(subset)\n",
    "\n",
    "    # Apply PCA with the optimal number of components\n",
    "    n_components = 5\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(scaled_columns)\n",
    "    reduced_data_dict[group_name] = reduced_data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the elbow method to determine the optimal number of clusters\n",
    "wcss = {}\n",
    "\n",
    "for k in range(1, 11):\n",
    "  for group_name, group in groups:\n",
    "    reduced_data = reduced_data_dict[group_name]\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(reduced_data)\n",
    "    \n",
    "    if group_name not in wcss:\n",
    "      wcss[group_name] = []\n",
    "\n",
    "    wcss[group_name].append(kmeans.inertia_)\n",
    "\n",
    "for group_name, values in wcss.items():\n",
    "  plt.plot(range(1, 11), values, label=group_name)\n",
    "  plt.xlabel('Number of clusters (k)')\n",
    "  plt.ylabel('Within-cluster sum of squares (WCSS)')\n",
    "  plt.legend()\n",
    "  plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gap statistic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gap(data, k):\n",
    "  \"\"\"\n",
    "  Compute the gap statistic for a given value of k.\n",
    "\n",
    "  Parameters:\n",
    "  - data: the data to cluster, with shape (n_samples, n_features)\n",
    "  - k: the number of clusters\n",
    "\n",
    "  Returns:\n",
    "  - gap: the gap statistic for the given value of k\n",
    "  \"\"\"\n",
    "  # Compute the WCSS for the real data\n",
    "  kmeans = KMeans(n_clusters=k, random_state=42).fit(data)\n",
    "  wcss = kmeans.inertia_\n",
    "\n",
    "  # Compute the null reference distribution by shuffling the data and\n",
    "  # re-assigning it to clusters\n",
    "  n_samples, n_features = data.shape\n",
    "  wcss_null = []\n",
    "\n",
    "  for _ in range(20):\n",
    "    data_shuffled = np.random.permutation(data)\n",
    "    wcss_null.append(KMeans(n_clusters=k).fit(data_shuffled).inertia_)\n",
    "  \n",
    "  wcss_null = np.array(wcss_null)\n",
    "  \n",
    "  # Compute the gap statistic and gap*\n",
    "  gap = np.log(np.mean(wcss_null)) - np.log(wcss)\n",
    "\n",
    "  # Compute the standard deviation of the null reference distribution\n",
    "  gap_std = np.std(np.log(wcss_null))\n",
    "\n",
    "  return gap, gap_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each group and generate scree plot\n",
    "optimal_num_k = {}\n",
    "optimal_num_k_2 = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "    # Initialize lists to store the gap statistics and error bars for different values of k\n",
    "    group_gaps = []\n",
    "    group_errors = []\n",
    "\n",
    "    # Loop over different values of k\n",
    "    for k in range(1, 11):\n",
    "        reduced_data = reduced_data_dict[group_name]\n",
    "\n",
    "        # Compute the gap statistic and standard deviation for the current value of k\n",
    "        gap, gap_std = compute_gap(reduced_data, k)\n",
    "        group_gaps.append(gap)\n",
    "        group_errors.append(gap_std)\n",
    "\n",
    "    # Find the optimal number of components based on the gap statistic criterion\n",
    "    optimal_k = None\n",
    "    for i in range(1, len(group_gaps) - 1):\n",
    "        s_k = group_errors[i]\n",
    "        threshold = s_k * np.sqrt(1 + 1 / 20)\n",
    "        if group_gaps[i] >= group_gaps[i + 1] - threshold:\n",
    "            optimal_k = i + 1\n",
    "            break\n",
    "\n",
    "    if optimal_k is None:\n",
    "        optimal_k = np.argmax(group_gaps) + 1\n",
    "\n",
    "    optimal_num_k[group_name] = optimal_k\n",
    "\n",
    "    # Find the optimal number of components based on the gap* statistic criterion\n",
    "    for i in range(1, len(group_gaps)):\n",
    "        optimal_k_2 = np.argmax(group_gaps) + 1\n",
    "\n",
    "    optimal_num_k_2[group_name] = optimal_k_2\n",
    "\n",
    "    # Plot the gap statistics\n",
    "    plt.plot(range(1, 11), group_gaps, label=group_name)\n",
    "\n",
    "    # Set labels and display the plot\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Gap statistic')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Group: {group_name}, Optimal k (gap): {optimal_num_k[group_name]}, Optimal k (gap*): {optimal_num_k_2[group_name]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe to store results\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# Loop over each group and apply KMeans with optimal number of clusters\n",
    "kmeans_dict = {}\n",
    "kmeans_2_dict = {}\n",
    "cluster_labels_dict = {}\n",
    "cluster_labels_2_dict = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "    # Get reduced data\n",
    "    reduced_data = reduced_data_dict[group_name]\n",
    "\n",
    "    # Get optimal number of clusters\n",
    "    k = optimal_num_k[group_name]\n",
    "    k_2 = optimal_num_k_2[group_name]\n",
    "\n",
    "    # Apply KMeans with optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(reduced_data)\n",
    "    kmeans_2 = KMeans(n_clusters=k_2, random_state=42).fit(reduced_data)\n",
    "    \n",
    "    # Store KMeans object in dictionary\n",
    "    kmeans_dict[group_name] = kmeans\n",
    "    kmeans_2_dict[group_name] = kmeans_2\n",
    "\n",
    "    cluster_labels_dict[group_name] = kmeans.labels_\n",
    "    cluster_labels_2_dict[group_name] = kmeans_2.labels_\n",
    "\n",
    "    # Print cluster labels\n",
    "    print(f\"Group {group_name}: {kmeans.labels_}\")\n",
    "\n",
    "    # Assign cluster labels to original group data\n",
    "    group = group.assign(cluster=kmeans.labels_)\n",
    "    group = group.assign(cluster_2=kmeans_2.labels_)\n",
    "\n",
    "    # Append group to original dataframe\n",
    "    df = pd.concat([df, group])\n",
    "    \n",
    "    # Plot clusters\n",
    "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title(group_name)\n",
    "    plt.show()\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby('playlist_name')\n",
    "\n",
    "shares_perc = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "    shares = group['cluster'].value_counts(normalize=True)\n",
    "    shares_perc[group_name] = shares * 100\n",
    "\n",
    "print(shares_perc)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HH-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby('playlist_name')\n",
    "\n",
    "hhi = {}\n",
    "hhi_2 = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "  shares = group['cluster'].value_counts(normalize=True)\n",
    "  hhi[group_name] = sum((shares*100)**2)\n",
    "\n",
    "  shares_2 = group['cluster_2'].value_counts(normalize=True)\n",
    "  hhi_2[group_name] = sum((shares_2*100)**2)\n",
    "\n",
    "print(hhi)\n",
    "print(hhi_2)\n",
    "\n",
    "print(\"Minimum HHI: \", min(hhi.values()))\n",
    "print(\"Mean HHI: \", np.mean(list(hhi.values())))\n",
    "print(\"Median HHI: \", np.median(list(hhi.values())))\n",
    "print(\"Maximum HHI: \", max(hhi.values()))\n",
    "print(\"Standard deviation: \", np.std(list(hhi.values())))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_dict = {}\n",
    "distances_dict_2 = {}\n",
    "\n",
    "# Loop over each group\n",
    "for group_name, group in groups:\n",
    "    # Get the cluster centroids for the group\n",
    "    cluster_centroids = kmeans_dict[group_name].cluster_centers_\n",
    "    cluster_centroids_2 = kmeans_2_dict[group_name].cluster_centers_\n",
    "\n",
    "    # Calculate pairwise euclidean distances between cluster centroids\n",
    "    distances = pairwise_distances(cluster_centroids)\n",
    "    distances_2 = pairwise_distances(cluster_centroids_2)\n",
    "\n",
    "    # Calculate mean pairwise distance\n",
    "    mean_distance = distances.mean()\n",
    "    mean_distance_2 = distances_2.mean()\n",
    "\n",
    "    # Print mean pairwise distance for the group\n",
    "    print(f\"Group {group_name}: Mean pairwise distance between cluster centroids = {mean_distance}\")\n",
    "\n",
    "    distances_dict[group_name] = mean_distance\n",
    "    distances_dict_2[group_name] = mean_distance_2\n",
    "\n",
    "# Calculate max, min, mean, median and standard deviation of distances\n",
    "distances_list = [d for distances in distances_dict.values() for d in distances.flatten() if not np.isnan(d)]\n",
    "print(f\"Max distance: {np.max(distances_list):.5f}\")\n",
    "print(f\"Min distance: {np.min(distances_list):.5f}\")\n",
    "print(f\"Mean distance: {np.mean(distances_list):.5f}\")\n",
    "print(f\"Median distance: {np.median(distances_list):.5f}\")\n",
    "print(f\"Standard deviation of distances: {np.std(distances_list):.5f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stirling diversity index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Stirling index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stirling_index_dict = {}\n",
    "\n",
    "# loop over each playlist and compute the diversity index\n",
    "for group_name, group in df.groupby('playlist_name'):\n",
    "    # compute the diversity index as in your original code\n",
    "    clusters = group.groupby('cluster')\n",
    "\n",
    "    total_index = 0\n",
    "\n",
    "    # Get the cluster centroids for the group\n",
    "    cluster_centroids = kmeans_dict[group_name].cluster_centers_\n",
    "    \n",
    "    for i, (cluster_i_name, cluster_i) in enumerate(clusters):\n",
    "        for j, (cluster_j_name, cluster_j) in enumerate(clusters):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            \n",
    "            centroid_i = cluster_centroids[i]\n",
    "            centroid_j = cluster_centroids[j]\n",
    "            dist = cdist([centroid_i], [centroid_j], 'euclidean')\n",
    "\n",
    "            share_i = len(cluster_i) / len(group)\n",
    "            share_j = len(cluster_j) / len(group)\n",
    "\n",
    "            index = dist * share_i * share_j\n",
    "\n",
    "            total_index += index\n",
    "\n",
    "    stirling_index_dict[group_name] = total_index\n",
    "\n",
    "print(stirling_index_dict)\n",
    "\n",
    "print(\"Minimum index: \", min(stirling_index_dict.values()))\n",
    "print(\"Mean index: \", np.mean(list(stirling_index_dict.values())))\n",
    "print(\"Median index: \", np.median(list(stirling_index_dict.values())))\n",
    "print(\"Maximum index: \", max(stirling_index_dict.values()))\n",
    "print(\"Standard deviation: \", np.std(list(stirling_index_dict.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stirling_index_2_dict = {}\n",
    "\n",
    "# loop over each playlist and compute the diversity index\n",
    "for group_name, group in df.groupby('playlist_name'):\n",
    "    # compute the diversity index as in your original code\n",
    "    clusters = group.groupby('cluster_2')\n",
    "\n",
    "    total_index = 0\n",
    "\n",
    "    # Get the cluster centroids for the group\n",
    "    cluster_centroids = kmeans_2_dict[group_name].cluster_centers_\n",
    "    \n",
    "    for i, (cluster_i_name, cluster_i) in enumerate(clusters):\n",
    "        for j, (cluster_j_name, cluster_j) in enumerate(clusters):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            \n",
    "            centroid_i = cluster_centroids[i]\n",
    "            centroid_j = cluster_centroids[j]\n",
    "            dist = cdist([centroid_i], [centroid_j], 'euclidean')\n",
    "\n",
    "            share_i = len(cluster_i) / len(group)\n",
    "            share_j = len(cluster_j) / len(group)\n",
    "\n",
    "            index = dist * share_i * share_j\n",
    "\n",
    "            total_index += index\n",
    "\n",
    "    stirling_index_2_dict[group_name] = total_index\n",
    "\n",
    "print(stirling_index_2_dict)\n",
    "\n",
    "print(\"Minimum index: \", min(stirling_index_2_dict.values()))\n",
    "print(\"Mean index: \", np.mean(list(stirling_index_2_dict.values())))\n",
    "print(\"Median index: \", np.median(list(stirling_index_2_dict.values())))\n",
    "print(\"Maximum index: \", max(stirling_index_2_dict.values()))\n",
    "print(\"Standard deviation: \", np.std(list(stirling_index_2_dict.values())))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted Stirling shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_shares = {}\n",
    "\n",
    "# loop over each playlist and compute the weighted Stirling index\n",
    "for group_name, group in df.groupby('playlist_name'):\n",
    "    # compute the weighted Stirling index\n",
    "    clusters = group.groupby('cluster')\n",
    "\n",
    "    total_index = 0\n",
    "\n",
    "    # Get the cluster centroids for the group\n",
    "    cluster_centroids = kmeans_dict[group_name].cluster_centers_\n",
    "\n",
    "    for i, (cluster_i_name, cluster_i) in enumerate(clusters):\n",
    "        for j, (cluster_j_name, cluster_j) in enumerate(clusters):\n",
    "            if i >= j:\n",
    "                continue\n",
    "\n",
    "            centroid_i = cluster_centroids[i]\n",
    "            centroid_j = cluster_centroids[j]\n",
    "            dist = cdist([centroid_i], [centroid_j], 'euclidean')\n",
    "\n",
    "            share_i = len(cluster_i) / len(group)\n",
    "            share_j = len(cluster_j) / len(group)\n",
    "\n",
    "            # compute the weighted Stirling index\n",
    "            alpha = 0.7  # weight for shares\n",
    "            beta = 0.3  # weight for distances\n",
    "\n",
    "            index = (share_i * share_j) ** alpha * dist ** beta\n",
    "\n",
    "            total_index += index\n",
    "\n",
    "    weighted_shares[group_name] = total_index\n",
    "\n",
    "print(weighted_shares)\n",
    "\n",
    "print(\"Minimum index: \", min(weighted_shares.values()))\n",
    "print(\"Mean index: \", np.mean(list(weighted_shares.values())))\n",
    "print(\"Median index: \", np.median(list(weighted_shares.values())))\n",
    "print(\"Maximum index: \", max(weighted_shares.values()))\n",
    "print(\"Standard deviation: \", np.std(list(weighted_shares.values())))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted Stirling dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe to store the results\n",
    "results_df = pd.DataFrame()\n",
    "\n",
    "# loop over each playlist and compute the weighted Stirling index\n",
    "for group_name, group in df.groupby('playlist_name'):\n",
    "    # compute the weighted Stirling index\n",
    "    clusters = group.groupby('cluster')\n",
    "\n",
    "    total_index = 0\n",
    "\n",
    "    # Get the cluster centroids for the group\n",
    "    cluster_centroids = kmeans_dict[group_name].cluster_centers_\n",
    "\n",
    "    for i, (cluster_i_name, cluster_i) in enumerate(clusters):\n",
    "        for j, (cluster_j_name, cluster_j) in enumerate(clusters):\n",
    "            if i >= j:\n",
    "                continue\n",
    "\n",
    "            centroid_i = cluster_centroids[i]\n",
    "            centroid_j = cluster_centroids[j]\n",
    "            dist = cdist([centroid_i], [centroid_j], 'euclidean')\n",
    "\n",
    "            share_i = len(cluster_i) / len(group)\n",
    "            share_j = len(cluster_j) / len(group)\n",
    "\n",
    "            # compute the weighted Stirling index\n",
    "            alpha = 0.3  # weight for shares\n",
    "            beta = 0.7  # weight for distances\n",
    "\n",
    "            index = (share_i * share_j) ** alpha * dist ** beta\n",
    "\n",
    "            total_index += index\n",
    "\n",
    "    # compute the number of different artists\n",
    "    nb_artists = len(group['artist_name'].unique())\n",
    "\n",
    "    # get the distances and HH index\n",
    "    distances = distances_dict[group_name]\n",
    "    distances_2 = distances_dict_2[group_name]\n",
    "    hh_index = hhi[group_name]\n",
    "    hh_index_2 = hhi_2[group_name]\n",
    "    stirling_index = stirling_index_dict[group_name]\n",
    "    stirling_shares = weighted_shares[group_name]\n",
    "    stirling_2 = stirling_index_2_dict[group_name]\n",
    "\n",
    "    # append the results to the new dataframe\n",
    "    playlist_followers = group['playlist_followers'].iloc[0]\n",
    "    track_popularity = group['popularity_track'].mean()\n",
    "    artist_popularity = group['popularity_artist'].mean()\n",
    "    nb_tracks = len(group)\n",
    "    nb_clusters = len(clusters)\n",
    "    nb_clusters_2 = optimal_num_k_2[group_name]\n",
    "\n",
    "    results_df = results_df.append({'playlist_name': group_name, 'playlist_followers': playlist_followers,\n",
    "                                    'track_pop': track_popularity, 'artist_pop': artist_popularity,\n",
    "                                    'nb_tracks': nb_tracks, 'nb_artists': nb_artists, 'nb_dimensions':n_components, \n",
    "                                    'nb_clusters': nb_clusters, 'nb_clusters_2': nb_clusters_2, \n",
    "                                    'hhi': hh_index, 'hhi_2': hh_index_2, 'mean_distance': distances, 'mean_distance_2': distances_2,\n",
    "                                    'stirling': stirling_index, \n",
    "                                    'stirling_shares': stirling_shares, 'stirling_dist': total_index, 'stirling_2': stirling_2},\n",
    "                                   ignore_index=True)\n",
    "\n",
    "# compute statistics on the diversity index for all playlists\n",
    "print(\"Min: \", results_df['stirling_dist'].min())\n",
    "print(\"Max: \", results_df['stirling_dist'].max())\n",
    "print(\"Median: \", results_df['stirling_dist'].median())\n",
    "print(\"Mean: \", results_df['stirling_dist'].mean())\n",
    "print(\"Standard deviation: \", results_df['stirling_dist'].std())\n",
    "\n",
    "# print the results dataframe\n",
    "print(results_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse final results into excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel(\"df_tops_final.xlsx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robustness checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('df_electro.xlsx')\n",
    "\n",
    "# Group by playlist name\n",
    "groups = df.groupby('playlist_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Initialize dictionaries to store clustering results\n",
    "reduced_data_dict_5 = {}\n",
    "reduced_data_dict_4 = {}\n",
    "\n",
    "# Loop over each group and apply PCA with the specified number of components\n",
    "for group_name, group in groups:\n",
    "    # Subset data\n",
    "    subset = group.loc[:, \"danceability\":\"duration_ms\"]\n",
    "\n",
    "    scaled_columns = scaler.fit_transform(subset)\n",
    "\n",
    "    # Apply PCA with 5 dimensions\n",
    "    pca_5 = PCA(n_components=5)\n",
    "    reduced_data_5 = pca_5.fit_transform(scaled_columns)\n",
    "\n",
    "    # Apply PCA with 4 dimensions\n",
    "    pca_4 = PCA(n_components=4)\n",
    "    reduced_data_4 = pca_4.fit_transform(scaled_columns)\n",
    "\n",
    "    reduced_data_dict_5[group_name] = reduced_data_5\n",
    "    reduced_data_dict_4[group_name] = reduced_data_4\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gap(data, k):\n",
    "  \"\"\"\n",
    "  Compute the gap statistic for a given value of k.\n",
    "\n",
    "  Parameters:\n",
    "  - data: the data to cluster, with shape (n_samples, n_features)\n",
    "  - k: the number of clusters\n",
    "\n",
    "  Returns:\n",
    "  - gap: the gap statistic for the given value of k\n",
    "  \"\"\"\n",
    "  # Compute the WCSS for the real data\n",
    "  kmeans = KMeans(n_clusters=k, random_state=42).fit(data)\n",
    "  wcss = kmeans.inertia_\n",
    "\n",
    "  # Compute the null reference distribution by shuffling the data and\n",
    "  # re-assigning it to clusters\n",
    "  n_samples, n_features = data.shape\n",
    "  wcss_null = []\n",
    "\n",
    "  for _ in range(20):\n",
    "    data_shuffled = np.random.permutation(data)\n",
    "    wcss_null.append(KMeans(n_clusters=k).fit(data_shuffled).inertia_)\n",
    "  \n",
    "  wcss_null = np.array(wcss_null)\n",
    "  \n",
    "  # Compute the gap statistic and gap*\n",
    "  gap = np.log(np.mean(wcss_null)) - np.log(wcss)\n",
    "\n",
    "  # Compute the standard deviation of the null reference distribution\n",
    "  gap_std = np.std(np.log(wcss_null))\n",
    "\n",
    "  return gap, gap_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each group and generate scree plot\n",
    "optimal_num_k_pca_5 = {}\n",
    "optimal_num_k_2_pca_5 = {}\n",
    "\n",
    "optimal_num_k_pca_4 = {}\n",
    "optimal_num_k_2_pca_4 = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "    # Initialize lists to store the gap statistics and error bars for different values of k\n",
    "    group_gaps_5 = []\n",
    "    group_errors_5 = []\n",
    "\n",
    "    group_gaps_4 = []\n",
    "    group_errors_4 = []\n",
    "\n",
    "    # Loop over different values of k\n",
    "    for k in range(1, 11):\n",
    "        reduced_data_5 = reduced_data_dict_5[group_name]\n",
    "        reduced_data_4 = reduced_data_dict_4[group_name]\n",
    "\n",
    "        # Compute the gap statistic and standard deviation for the current value of k\n",
    "        gap_5, gap_std_5 = compute_gap(reduced_data_5, k)\n",
    "        group_gaps_5.append(gap_5)\n",
    "        group_errors_5.append(gap_std_5)\n",
    "\n",
    "        gap_4, gap_std_4 = compute_gap(reduced_data_4, k)\n",
    "        group_gaps_4.append(gap_4)\n",
    "        group_errors_4.append(gap_std_4)\n",
    "\n",
    "    # Find the optimal number of components based on the gap statistic criterion\n",
    "    optimal_k_5 = None\n",
    "    for i in range(1, len(group_gaps_5) - 1):\n",
    "        s_k = group_errors_5[i]\n",
    "        threshold = s_k * np.sqrt(1 + 1 / 20)\n",
    "        if group_gaps_5[i] >= group_gaps_5[i + 1] - threshold:\n",
    "            optimal_k_5 = i + 1\n",
    "            break\n",
    "\n",
    "    if optimal_k_5 is None:\n",
    "        optimal_k_5 = np.argmax(group_gaps_5) + 1\n",
    "\n",
    "    optimal_num_k_pca_5[group_name] = optimal_k_5\n",
    "\n",
    "    optimal_k_4 = None\n",
    "    for i in range(1, len(group_gaps_4) - 1):\n",
    "        s_k = group_errors_4[i]\n",
    "        threshold = s_k * np.sqrt(1 + 1 / 20)\n",
    "        if group_gaps_4[i] >= group_gaps_4[i + 1] - threshold:\n",
    "            optimal_k_4 = i + 1\n",
    "            break\n",
    "\n",
    "    if optimal_k_4 is None:\n",
    "        optimal_k_4 = np.argmax(group_gaps_4) + 1\n",
    "\n",
    "    optimal_num_k_pca_5[group_name] = optimal_k_5\n",
    "    optimal_num_k_pca_4[group_name] = optimal_k_4\n",
    "\n",
    "    # Find the optimal number of components based on the gap* statistic criterion\n",
    "    optimal_k_2_5 = np.argmax(group_gaps_5) + 1\n",
    "    optimal_k_2_4 = np.argmax(group_gaps_4) + 1\n",
    "\n",
    "    optimal_num_k_2_pca_5[group_name] = optimal_k_2_5\n",
    "    optimal_num_k_2_pca_4[group_name] = optimal_k_2_4\n",
    "\n",
    "    print(f\"Group: {group_name}, Optimal k 5 (gap): {optimal_num_k_pca_5[group_name]}, Optimal k 4 (gap*): {optimal_num_k_2_pca_5[group_name]} and Optimal k 4 (gap): {optimal_num_k_pca_4[group_name]}, Optimal k 4 (gap*): {optimal_num_k_2_pca_4[group_name]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame()\n",
    "\n",
    "# Loop over each group and apply PCA with the specified number of components\n",
    "for group_name, group in groups:\n",
    "    # Get reduced data\n",
    "    reduced_data_5 = reduced_data_dict_5[group_name]\n",
    "    reduced_data_4 = reduced_data_dict_4[group_name]\n",
    "\n",
    "    # Perform k-means clustering for both configurations\n",
    "    kmeans_5 = KMeans(n_clusters=optimal_num_k_pca_5[group_name], random_state=42).fit(reduced_data_5)\n",
    "    kmeans_2_5 = KMeans(n_clusters=optimal_num_k_2_pca_5[group_name], random_state=42).fit(reduced_data_5)\n",
    "\n",
    "    kmeans_4 = KMeans(n_clusters=optimal_num_k_pca_4[group_name], random_state=42).fit(reduced_data_4)\n",
    "    kmeans_2_4 = KMeans(n_clusters=optimal_num_k_2_pca_4[group_name], random_state=42).fit(reduced_data_4)\n",
    "\n",
    "    # Get cluster assignments for both configurations\n",
    "    cluster_assignments_5 = kmeans_5.labels_\n",
    "    cluster_assignments_2_5 = kmeans_2_5.labels_\n",
    "\n",
    "    cluster_assignments_4 = kmeans_4.labels_\n",
    "    cluster_assignments_2_4 = kmeans_2_4.labels_\n",
    "\n",
    "    # Compute silhouette scores for both configurations\n",
    "    silhouette_score_5 = silhouette_score(reduced_data_5, cluster_assignments_5)\n",
    "    silhouette_score_2_5 = silhouette_score(reduced_data_5, cluster_assignments_2_5)\n",
    "\n",
    "    silhouette_score_4 = silhouette_score(reduced_data_4, cluster_assignments_4)\n",
    "    silhouette_score_2_4 = silhouette_score(reduced_data_4, cluster_assignments_2_4)\n",
    "\n",
    "    results_df = results_df.append({\n",
    "        'playlist_name': group_name,\n",
    "        'nb_cluster_5': optimal_num_k_pca_5[group_name],\n",
    "        'nb_cluster_5_2': optimal_num_k_2_pca_5[group_name],\n",
    "        'silhouette_score_5': silhouette_score_5,\n",
    "        'silhouette_score_5_2': silhouette_score_2_5,\n",
    "        'nb_cluster_4': optimal_num_k_pca_4[group_name],\n",
    "        'nb_cluster_4_2': optimal_num_k_2_pca_4[group_name],\n",
    "        'silhouette_score_4': silhouette_score_4,\n",
    "        'silhouette_score_4_2': silhouette_score_2_4\n",
    "    }, ignore_index=True)\n",
    "\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel('robust_electro.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0da26d773aaff631c161613594b6de0a8522876018f5987f12f3dad8e2ed0c84"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
