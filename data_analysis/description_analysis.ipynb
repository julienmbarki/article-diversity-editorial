{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import pandas as pd\n",
    "\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API connexion\n",
    "client_id = 'cec8979c027344f98b471a991aa415ad'\n",
    "client_secret = 'ad69bcba55b349f98c1344b006c708bd'\n",
    "\n",
    "client_credentials_manager = SpotifyClientCredentials(client_id, client_secret)\n",
    "sp = spotipy.Spotify(client_credentials_manager=client_credentials_manager)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get playlists description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_excel('/Users/julienmbarki/Documents/Doctorat/Publications/Article 2/Data/Code/data_requests/playlists_info_clean.xlsx')\n",
    "\n",
    "# Remove playlists with \"This is\" and \"Written By\" in the name\n",
    "df = df[~df['playlist_name'].str.contains('This Is', na=False)]\n",
    "df = df[~df['playlist_name'].str.contains('Written By', na=False)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API scraping\n",
    "playlists_ids = df['playlist_id'].tolist()\n",
    "\n",
    "playlists = []\n",
    "\n",
    "for playlist_id in playlists_ids:\n",
    "    print(f\"Processing playlist: {playlist_id}\")\n",
    "\n",
    "    playlists_results = sp.playlist(playlist_id)\n",
    "\n",
    "    playlist_name = playlists_results['name']\n",
    "    playlist_description = playlists_results['description']\n",
    "\n",
    "    playlists.append({\n",
    "        \"playlist_id\": playlist_id, \n",
    "        \"playlist_name\": playlist_name,\n",
    "        \"playlist_description\": playlist_description\n",
    "    })\n",
    "\n",
    "# Create dataframe from playlists\n",
    "df_playlists = pd.DataFrame(playlists, columns=[\"playlist_id\", \"playlist_name\", \"playlist_description\"])\n",
    "df_playlists.to_excel(\"playlists_descriptions.xlsx\", index=False)\n",
    "\n",
    "print(df_playlists)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process playlists descrption with text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_excel('playlists_descriptions.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/julienmbarki/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/julienmbarki/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/julienmbarki/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/julienmbarki/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.016*\"sound\" + 0.014*\"playlist\" + 0.010*\"music\" + 0.010*\"listen\" + 0.007*\"song\"\n",
      "Topic 1: 0.017*\"song\" + 0.014*\"music\" + 0.008*\"piano\" + 0.007*\"netflix\" + 0.007*\"instrumental\"\n",
      "Topic 2: 0.021*\"music\" + 0.018*\"new\" + 0.015*\"best\" + 0.009*\"hit\" + 0.008*\"rock\"\n",
      "Topic 3: 0.064*\"track\" + 0.056*\"right\" + 0.052*\"update\" + 0.035*\"played\" + 0.034*\"daily\"\n",
      "                                   playlist_description  dominant_topic\n",
      "0     The hottest Afropop, Afrobeats and Afro-Caribb...               0\n",
      "1     Tracks popping off in the Afro scene. Cover: Q...               1\n",
      "2     Le meilleur du rap africain francophone. Photo...               0\n",
      "3     The tracks heating up the continent right now!...               3\n",
      "4     Les plus belles voix de la musique africaine. ...               3\n",
      "...                                                 ...             ...\n",
      "1560  Your daily update of the most viral tracks rig...               3\n",
      "1561  Your daily update of the most viral tracks rig...               3\n",
      "1562  Your daily update of the most viral tracks rig...               3\n",
      "1563  Your daily update of the most viral tracks rig...               3\n",
      "1564  Your daily update of the most viral tracks rig...               3\n",
      "\n",
      "[1565 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Step 1: Preprocess text\n",
    "# Combine English and French stopwords and add custom words\n",
    "custom_stopwords = set(stopwords.words('english'))  # Default English stopwords\n",
    "custom_stopwords.update(stopwords.words('french'))  # Add French stopwords\n",
    "custom_stopwords.update(['cover', 'photo'])  # Add specific words to exclude\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the text is a string\n",
    "        tokens = word_tokenize(text.lower())  # Lowercase and tokenize\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalpha()]  # Remove non-alphabetic tokens\n",
    "        tokens = [word for word in tokens if word not in custom_stopwords]  # Remove stopwords\n",
    "        return tokens\n",
    "    else:\n",
    "        return []  # Return an empty list if the text is not a string (e.g., NaN)\n",
    "\n",
    "df['processed_description'] = df['playlist_description'].apply(preprocess_text)\n",
    "\n",
    "# Step 2: Create a dictionary and corpus\n",
    "dictionary = corpora.Dictionary(df['processed_description'])\n",
    "corpus = [dictionary.doc2bow(text) for text in df['processed_description']]\n",
    "\n",
    "# Step 3: Apply LDA model\n",
    "num_topics = 4  # Number of topics to extract\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=0)\n",
    "\n",
    "# Display topics and top words in each topic\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for idx, topic in topics:\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Step 4: Assign the dominant topic to each description\n",
    "def get_dominant_topic(text):\n",
    "    bow = dictionary.doc2bow(text)\n",
    "    topic_probs = lda_model.get_document_topics(bow)\n",
    "    dominant_topic = max(topic_probs, key=lambda x: x[1])[0]\n",
    "    return dominant_topic\n",
    "\n",
    "df['dominant_topic'] = df['processed_description'].apply(get_dominant_topic)\n",
    "\n",
    "# Display the DataFrame with dominant topics\n",
    "print(df[['playlist_description', 'dominant_topic']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge with panel data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
