{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import pairwise_distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_excel('/Users/julienmbarki/Documents/Doctorat/Publications/Article 2/Data/Code/data_requests/df_voyage.xlsx')\n",
    "\n",
    "# Group by playlist name\n",
    "groups = df.groupby('playlist_name')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA dimensionnality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the number of components\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "optimal_num_components = []\n",
    "\n",
    "for group_name, group in groups:\n",
    "    # Subset and scale\n",
    "    subset = group.loc[:, \"danceability\":\"time_signature\"]\n",
    "    scaled_columns = scaler.fit_transform(subset)\n",
    "\n",
    "    # Apply PCA\n",
    "    pca = PCA()\n",
    "    pca.fit(scaled_columns)\n",
    "    \n",
    "    # Calculate cumulative explained variance\n",
    "    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "    optimal_components = next(i for i, var in enumerate(cumulative_variance) if var >= 0.8) + 1\n",
    "    optimal_num_components.append(optimal_components)\n",
    "\n",
    "    # Plot scree plot\n",
    "    plt.plot(range(1, pca.n_components_ + 1), cumulative_variance, 'bo-', linewidth=2)\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title(group_name)\n",
    "    plt.show()\n",
    "\n",
    "# Calculate the most modal value\n",
    "most_common_optimal = Counter(optimal_num_components).most_common(1)\n",
    "most_modal_value = most_common_optimal[0][0]\n",
    "\n",
    "print(\"Most modal value of optimal components:\", most_modal_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce dimensionality\n",
    "# Loop over each group and apply PCA with optimal number of components\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "reduced_data_dict = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "    # Subset data\n",
    "    subset = group.loc[:, \"danceability\":\"time_signature\"]\n",
    "\n",
    "    # Scale the specified columns\n",
    "    scaled_columns = scaler.fit_transform(subset)\n",
    "\n",
    "    # Apply PCA with the optimal number of components\n",
    "    n_components = 6\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_data = pca.fit_transform(scaled_columns)\n",
    "    reduced_data_dict[group_name] = reduced_data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the elbow method to determine the optimal number of clusters\n",
    "# Calculate the elbow\n",
    "wcss = {}\n",
    "\n",
    "for k in range(1, 11):\n",
    "  for group_name, group in groups:\n",
    "    reduced_data = reduced_data_dict[group_name]\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(reduced_data)\n",
    "    \n",
    "    if group_name not in wcss:\n",
    "      wcss[group_name] = []\n",
    "\n",
    "    wcss[group_name].append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow\n",
    "for group_name, values in wcss.items():\n",
    "  plt.plot(range(1, 11), values, label=group_name)\n",
    "  plt.xlabel('Number of clusters (k)')\n",
    "  plt.ylabel('Within-cluster sum of squares (WCSS)')\n",
    "  plt.legend()\n",
    "  plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gap statistic method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute the gap statistic\n",
    "def compute_gap(data, k):\n",
    "  # Compute the WCSS for the real data\n",
    "  kmeans = KMeans(n_clusters=k, random_state=42).fit(data)\n",
    "  wcss = kmeans.inertia_\n",
    "\n",
    "  # Compute the null reference distribution by shuffling the data and\n",
    "  # re-assigning it to clusters\n",
    "  n_samples, n_features = data.shape\n",
    "  wcss_null = []\n",
    "\n",
    "  for _ in range(20):\n",
    "    data_shuffled = np.random.permutation(data)\n",
    "    wcss_null.append(KMeans(n_clusters=k).fit(data_shuffled).inertia_)\n",
    "  \n",
    "  wcss_null = np.array(wcss_null)\n",
    "  \n",
    "  # Compute the gap statistic and gap*\n",
    "  gap = np.mean(np.log(wcss_null)) - np.log(wcss)\n",
    "\n",
    "  # Compute the standard deviation of the null reference distribution\n",
    "  gap_std = np.std(np.log(wcss_null))\n",
    "\n",
    "  return gap, gap_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the optimal k\n",
    "# Loop over each group and generate scree plot\n",
    "optimal_num_k = {}\n",
    "optimal_num_k_2 = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "    group_gaps = []\n",
    "    group_errors = []\n",
    "\n",
    "    for k in range(1, 11):\n",
    "        reduced_data = reduced_data_dict[group_name]\n",
    "\n",
    "        # Compute the gap statistic and standard deviation for the current value of k\n",
    "        gap, gap_std = compute_gap(reduced_data, k)\n",
    "        group_gaps.append(gap)\n",
    "        group_errors.append(gap_std)\n",
    "\n",
    "    # Find the optimal number of components based on the gap statistic criterion\n",
    "    optimal_k = None\n",
    "    for i in range(1, len(group_gaps) - 1):\n",
    "        s_k = group_errors[i]\n",
    "        threshold = s_k * np.sqrt(1 + 1 / 20)\n",
    "        if group_gaps[i] >= group_gaps[i + 1] - threshold:\n",
    "            optimal_k = i + 1\n",
    "            break\n",
    "\n",
    "    if optimal_k is None:\n",
    "        optimal_k = np.argmax(group_gaps) + 1\n",
    "\n",
    "    optimal_num_k[group_name] = optimal_k\n",
    "\n",
    "    # Find the optimal number of components based on the gap* statistic criterion\n",
    "    for i in range(1, len(group_gaps)):\n",
    "        optimal_k_2 = np.argmax(group_gaps) + 1\n",
    "\n",
    "    optimal_num_k_2[group_name] = optimal_k_2\n",
    "\n",
    "    # Plot the gap statistics\n",
    "    plt.plot(range(1, 11), group_gaps, label=group_name)\n",
    "\n",
    "    # Set labels and display the plot\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('Gap statistic')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Group: {group_name}, Optimal k (gap): {optimal_num_k[group_name]}, Optimal k (gap*): {optimal_num_k_2[group_name]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply KMeans with optimal number of clusters\n",
    "df = pd.DataFrame()\n",
    "\n",
    "kmeans_dict = {}\n",
    "kmeans_2_dict = {}\n",
    "cluster_labels_dict = {}\n",
    "cluster_labels_2_dict = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "    reduced_data = reduced_data_dict[group_name]\n",
    "\n",
    "    k = optimal_num_k[group_name]\n",
    "    k_2 = optimal_num_k_2[group_name]\n",
    "\n",
    "    # Apply KMeans with optimal number of clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(reduced_data)\n",
    "    kmeans_2 = KMeans(n_clusters=k_2, random_state=42).fit(reduced_data)\n",
    "    \n",
    "    kmeans_dict[group_name] = kmeans\n",
    "    kmeans_2_dict[group_name] = kmeans_2\n",
    "\n",
    "    cluster_labels_dict[group_name] = kmeans.labels_\n",
    "    cluster_labels_2_dict[group_name] = kmeans_2.labels_\n",
    "\n",
    "    print(f\"Group {group_name}: {kmeans.labels_}\")\n",
    "\n",
    "    # Assign cluster labels to original group data\n",
    "    group = group.assign(cluster=kmeans.labels_)\n",
    "    group = group.assign(cluster_2=kmeans_2.labels_)\n",
    "\n",
    "    # Append group to original dataframe\n",
    "    df = pd.concat([df, group])\n",
    "    \n",
    "    # Plot clusters\n",
    "    plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.title(group_name)\n",
    "    plt.show()\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HH-Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = df.groupby('playlist_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the shares of each cluster\n",
    "shares_perc = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "    shares = group['cluster'].value_counts(normalize=True)\n",
    "    shares_perc[group_name] = shares * 100\n",
    "\n",
    "# Calculate the HHI\n",
    "hhi = {}\n",
    "hhi_2 = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "  shares = group['cluster'].value_counts(normalize=True)\n",
    "  hhi[group_name] = sum((shares*100)**2)\n",
    "\n",
    "  shares_2 = group['cluster_2'].value_counts(normalize=True)\n",
    "  hhi_2[group_name] = sum((shares_2*100)**2)\n",
    "\n",
    "print(hhi)\n",
    "print(hhi_2)\n",
    "\n",
    "print(\"Minimum HHI: \", min(hhi.values()))\n",
    "print(\"Mean HHI: \", np.mean(list(hhi.values())))\n",
    "print(\"Median HHI: \", np.median(list(hhi.values())))\n",
    "print(\"Maximum HHI: \", max(hhi.values()))\n",
    "print(\"Standard deviation: \", np.std(list(hhi.values())))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances between cluster centroids\n",
    "distances_dict = {}\n",
    "distances_dict_2 = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "    # Get the cluster centroids for the group\n",
    "    cluster_centroids = kmeans_dict[group_name].cluster_centers_\n",
    "    cluster_centroids_2 = kmeans_2_dict[group_name].cluster_centers_\n",
    "\n",
    "    # Calculate pairwise euclidean distances between cluster centroids\n",
    "    distances = pairwise_distances(cluster_centroids)\n",
    "    distances_2 = pairwise_distances(cluster_centroids_2)\n",
    "\n",
    "    # Calculate mean pairwise distance\n",
    "    mean_distance = distances.mean()\n",
    "    mean_distance_2 = distances_2.mean()\n",
    "\n",
    "    # Print mean pairwise distance for the group\n",
    "    print(f\"Group {group_name}: Mean pairwise distance between cluster centroids = {mean_distance}\")\n",
    "\n",
    "    distances_dict[group_name] = mean_distance\n",
    "    distances_dict_2[group_name] = mean_distance_2\n",
    "\n",
    "# Calculate max, min, mean, median and standard deviation of distances\n",
    "distances_list = [d for distances in distances_dict.values() for d in distances.flatten() if not np.isnan(d)]\n",
    "print(f\"Max distance: {np.max(distances_list):.5f}\")\n",
    "print(f\"Min distance: {np.min(distances_list):.5f}\")\n",
    "print(f\"Mean distance: {np.mean(distances_list):.5f}\")\n",
    "print(f\"Median distance: {np.median(distances_list):.5f}\")\n",
    "print(f\"Standard deviation of distances: {np.std(distances_list):.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distances bewteen tracks\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "distances_dict_3 = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "    subset = group.loc[:, \"danceability\":\"time_signature\"]\n",
    "    scaled_columns = scaler.fit_transform(subset)\n",
    "\n",
    "    pairwise_dist = pairwise_distances(scaled_columns)\n",
    "    mean_distance = pairwise_dist.mean()\n",
    "    \n",
    "    # Print mean pairwise distance for the group\n",
    "    print(f\"Group {group_name}: Mean pairwise distance between tracks = {mean_distance}\")\n",
    "\n",
    "    distances_dict_3[group_name] = mean_distance\n",
    "\n",
    "# Calculate max, min, mean, median and standard deviation of distances\n",
    "distances_list = [d for distances in distances_dict_3.values() for d in distances.flatten() if not np.isnan(d)]\n",
    "print(f\"Max distance: {np.max(distances_list):.5f}\")\n",
    "print(f\"Min distance: {np.min(distances_list):.5f}\")\n",
    "print(f\"Mean distance: {np.mean(distances_list):.5f}\")\n",
    "print(f\"Median distance: {np.median(distances_list):.5f}\")\n",
    "print(f\"Standard deviation of distances: {np.std(distances_list):.5f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stirling diversity index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-alpha Rao-Stirling index\n",
    "stirling_index_dict = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "    clusters = group.groupby('cluster')\n",
    "\n",
    "    total_index = 0\n",
    "\n",
    "    cluster_centroids = kmeans_dict[group_name].cluster_centers_\n",
    "    \n",
    "    for i, (cluster_i_name, cluster_i) in enumerate(clusters):\n",
    "        for j, (cluster_j_name, cluster_j) in enumerate(clusters):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            \n",
    "            centroid_i = cluster_centroids[i]\n",
    "            centroid_j = cluster_centroids[j]\n",
    "            dist = cdist([centroid_i], [centroid_j], 'euclidean')\n",
    "\n",
    "            share_i = len(cluster_i) / len(group)\n",
    "            share_j = len(cluster_j) / len(group)\n",
    "\n",
    "            index = dist * share_i * share_j\n",
    "\n",
    "            total_index += index\n",
    "\n",
    "    stirling_index_dict[group_name] = total_index\n",
    "\n",
    "print(stirling_index_dict)\n",
    "\n",
    "print(\"Minimum index: \", min(stirling_index_dict.values()))\n",
    "print(\"Mean index: \", np.mean(list(stirling_index_dict.values())))\n",
    "print(\"Median index: \", np.median(list(stirling_index_dict.values())))\n",
    "print(\"Maximum index: \", max(stirling_index_dict.values()))\n",
    "print(\"Standard deviation: \", np.std(list(stirling_index_dict.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-beta Rao-Stirling index\n",
    "stirling_index_2_dict = {}\n",
    "\n",
    "for group_name, group in groups:\n",
    "    clusters = group.groupby('cluster_2')\n",
    "\n",
    "    total_index = 0\n",
    "\n",
    "    cluster_centroids = kmeans_2_dict[group_name].cluster_centers_\n",
    "    \n",
    "    for i, (cluster_i_name, cluster_i) in enumerate(clusters):\n",
    "        for j, (cluster_j_name, cluster_j) in enumerate(clusters):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            \n",
    "            centroid_i = cluster_centroids[i]\n",
    "            centroid_j = cluster_centroids[j]\n",
    "            dist = cdist([centroid_i], [centroid_j], 'euclidean')\n",
    "\n",
    "            share_i = len(cluster_i) / len(group)\n",
    "            share_j = len(cluster_j) / len(group)\n",
    "\n",
    "            index = dist * share_i * share_j\n",
    "\n",
    "            total_index += index\n",
    "\n",
    "    stirling_index_2_dict[group_name] = total_index\n",
    "\n",
    "print(stirling_index_2_dict)\n",
    "\n",
    "print(\"Minimum index: \", min(stirling_index_2_dict.values()))\n",
    "print(\"Mean index: \", np.mean(list(stirling_index_2_dict.values())))\n",
    "print(\"Median index: \", np.median(list(stirling_index_2_dict.values())))\n",
    "print(\"Maximum index: \", max(stirling_index_2_dict.values()))\n",
    "print(\"Standard deviation: \", np.std(list(stirling_index_2_dict.values())))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append playlist-level indicators\n",
    "panel_data = []\n",
    "\n",
    "for group_name, group in groups:\n",
    "    # Retrieve playlist-level indicators\n",
    "    # Variety\n",
    "    nb_clusters = optimal_num_k[group_name]\n",
    "    nb_clusters_2 = optimal_num_k_2[group_name]\n",
    "\n",
    "    # Balance\n",
    "    hh_index = hhi[group_name]\n",
    "    hh_index_2 = hhi_2[group_name]\n",
    "\n",
    "    # Distparity\n",
    "    distances = distances_dict[group_name]\n",
    "    distances_2 = distances_dict_2[group_name]\n",
    "    distances_3 = distances_dict_3[group_name]\n",
    "\n",
    "    # Diversity\n",
    "    stirling_index = stirling_index_dict[group_name]\n",
    "    stirling_index_2 = stirling_index_2_dict[group_name]\n",
    "    \n",
    "    # Add playlist-level indicators to each track in the playlist\n",
    "    for _, track in group.iterrows():\n",
    "        track_data = track.to_dict()\n",
    "        track_data.update({\n",
    "            'nb_clusters': nb_clusters,\n",
    "            'nb_clusters_2': nb_clusters_2,\n",
    "            'hh_index': hh_index,\n",
    "            'hh_index_2': hh_index_2,\n",
    "            'distances': distances,\n",
    "            'distances_2': distances_2,\n",
    "            'distances_3': distances_3,\n",
    "            'stirling_index': stirling_index,\n",
    "            'stirling_index_2': stirling_index_2\n",
    "        })\n",
    "        panel_data.append(track_data)\n",
    "\n",
    "# Convert to DataFrame\n",
    "panel_data_df = pd.DataFrame(panel_data)\n",
    "display(panel_data_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Excel\n",
    "panel_data_df.to_excel(\"df_voyage_final.xlsx\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
